{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-17</th>\n",
       "      <td>465.864014</td>\n",
       "      <td>468.174011</td>\n",
       "      <td>452.421997</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>21056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-18</th>\n",
       "      <td>456.859985</td>\n",
       "      <td>456.859985</td>\n",
       "      <td>413.104004</td>\n",
       "      <td>424.440002</td>\n",
       "      <td>424.440002</td>\n",
       "      <td>34483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-19</th>\n",
       "      <td>424.102997</td>\n",
       "      <td>427.834991</td>\n",
       "      <td>384.532013</td>\n",
       "      <td>394.795990</td>\n",
       "      <td>394.795990</td>\n",
       "      <td>37919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-20</th>\n",
       "      <td>394.673004</td>\n",
       "      <td>423.295990</td>\n",
       "      <td>389.882996</td>\n",
       "      <td>408.903992</td>\n",
       "      <td>408.903992</td>\n",
       "      <td>36863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-21</th>\n",
       "      <td>408.084991</td>\n",
       "      <td>412.425995</td>\n",
       "      <td>393.181000</td>\n",
       "      <td>398.821014</td>\n",
       "      <td>398.821014</td>\n",
       "      <td>26580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>20686.746094</td>\n",
       "      <td>21163.011719</td>\n",
       "      <td>20685.380859</td>\n",
       "      <td>21086.792969</td>\n",
       "      <td>21086.792969</td>\n",
       "      <td>21152848261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-20</th>\n",
       "      <td>21085.373047</td>\n",
       "      <td>22692.357422</td>\n",
       "      <td>20919.126953</td>\n",
       "      <td>22676.552734</td>\n",
       "      <td>22676.552734</td>\n",
       "      <td>28799154319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-21</th>\n",
       "      <td>22677.427734</td>\n",
       "      <td>23282.347656</td>\n",
       "      <td>22511.833984</td>\n",
       "      <td>22777.625000</td>\n",
       "      <td>22777.625000</td>\n",
       "      <td>32442278429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-22</th>\n",
       "      <td>22777.986328</td>\n",
       "      <td>23056.730469</td>\n",
       "      <td>22387.900391</td>\n",
       "      <td>22720.416016</td>\n",
       "      <td>22720.416016</td>\n",
       "      <td>24746386230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-23</th>\n",
       "      <td>22700.687500</td>\n",
       "      <td>22973.865234</td>\n",
       "      <td>22673.906250</td>\n",
       "      <td>22858.593750</td>\n",
       "      <td>22858.593750</td>\n",
       "      <td>26985844736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3051 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  \\\n",
       "Date                                                                 \n",
       "2014-09-17    465.864014    468.174011    452.421997    457.334015   \n",
       "2014-09-18    456.859985    456.859985    413.104004    424.440002   \n",
       "2014-09-19    424.102997    427.834991    384.532013    394.795990   \n",
       "2014-09-20    394.673004    423.295990    389.882996    408.903992   \n",
       "2014-09-21    408.084991    412.425995    393.181000    398.821014   \n",
       "...                  ...           ...           ...           ...   \n",
       "2023-01-19  20686.746094  21163.011719  20685.380859  21086.792969   \n",
       "2023-01-20  21085.373047  22692.357422  20919.126953  22676.552734   \n",
       "2023-01-21  22677.427734  23282.347656  22511.833984  22777.625000   \n",
       "2023-01-22  22777.986328  23056.730469  22387.900391  22720.416016   \n",
       "2023-01-23  22700.687500  22973.865234  22673.906250  22858.593750   \n",
       "\n",
       "               Adj Close       Volume  \n",
       "Date                                   \n",
       "2014-09-17    457.334015     21056800  \n",
       "2014-09-18    424.440002     34483200  \n",
       "2014-09-19    394.795990     37919700  \n",
       "2014-09-20    408.903992     36863600  \n",
       "2014-09-21    398.821014     26580100  \n",
       "...                  ...          ...  \n",
       "2023-01-19  21086.792969  21152848261  \n",
       "2023-01-20  22676.552734  28799154319  \n",
       "2023-01-21  22777.625000  32442278429  \n",
       "2023-01-22  22720.416016  24746386230  \n",
       "2023-01-23  22858.593750  26985844736  \n",
       "\n",
       "[3051 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 1: Data cleaning\n",
    "data = yf.download(tickers='BTC-usd', period='max', interval='1d')\n",
    "data.dropna(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-17</th>\n",
       "      <td>465.864014</td>\n",
       "      <td>468.174011</td>\n",
       "      <td>452.421997</td>\n",
       "      <td>457.334015</td>\n",
       "      <td>21056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-18</th>\n",
       "      <td>456.859985</td>\n",
       "      <td>456.859985</td>\n",
       "      <td>413.104004</td>\n",
       "      <td>424.440002</td>\n",
       "      <td>34483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-19</th>\n",
       "      <td>424.102997</td>\n",
       "      <td>427.834991</td>\n",
       "      <td>384.532013</td>\n",
       "      <td>394.795990</td>\n",
       "      <td>37919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-20</th>\n",
       "      <td>394.673004</td>\n",
       "      <td>423.295990</td>\n",
       "      <td>389.882996</td>\n",
       "      <td>408.903992</td>\n",
       "      <td>36863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-21</th>\n",
       "      <td>408.084991</td>\n",
       "      <td>412.425995</td>\n",
       "      <td>393.181000</td>\n",
       "      <td>398.821014</td>\n",
       "      <td>26580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>20686.746094</td>\n",
       "      <td>21163.011719</td>\n",
       "      <td>20685.380859</td>\n",
       "      <td>21086.792969</td>\n",
       "      <td>21152848261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-20</th>\n",
       "      <td>21085.373047</td>\n",
       "      <td>22692.357422</td>\n",
       "      <td>20919.126953</td>\n",
       "      <td>22676.552734</td>\n",
       "      <td>28799154319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-21</th>\n",
       "      <td>22677.427734</td>\n",
       "      <td>23282.347656</td>\n",
       "      <td>22511.833984</td>\n",
       "      <td>22777.625000</td>\n",
       "      <td>32442278429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-22</th>\n",
       "      <td>22777.986328</td>\n",
       "      <td>23056.730469</td>\n",
       "      <td>22387.900391</td>\n",
       "      <td>22720.416016</td>\n",
       "      <td>24746386230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-23</th>\n",
       "      <td>22700.687500</td>\n",
       "      <td>22973.865234</td>\n",
       "      <td>22673.906250</td>\n",
       "      <td>22858.593750</td>\n",
       "      <td>26985844736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3051 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low     Adj Close  \\\n",
       "Date                                                                 \n",
       "2014-09-17    465.864014    468.174011    452.421997    457.334015   \n",
       "2014-09-18    456.859985    456.859985    413.104004    424.440002   \n",
       "2014-09-19    424.102997    427.834991    384.532013    394.795990   \n",
       "2014-09-20    394.673004    423.295990    389.882996    408.903992   \n",
       "2014-09-21    408.084991    412.425995    393.181000    398.821014   \n",
       "...                  ...           ...           ...           ...   \n",
       "2023-01-19  20686.746094  21163.011719  20685.380859  21086.792969   \n",
       "2023-01-20  21085.373047  22692.357422  20919.126953  22676.552734   \n",
       "2023-01-21  22677.427734  23282.347656  22511.833984  22777.625000   \n",
       "2023-01-22  22777.986328  23056.730469  22387.900391  22720.416016   \n",
       "2023-01-23  22700.687500  22973.865234  22673.906250  22858.593750   \n",
       "\n",
       "                 Volume  \n",
       "Date                     \n",
       "2014-09-17     21056800  \n",
       "2014-09-18     34483200  \n",
       "2014-09-19     37919700  \n",
       "2014-09-20     36863600  \n",
       "2014-09-21     26580100  \n",
       "...                 ...  \n",
       "2023-01-19  21152848261  \n",
       "2023-01-20  28799154319  \n",
       "2023-01-21  32442278429  \n",
       "2023-01-22  24746386230  \n",
       "2023-01-23  26985844736  \n",
       "\n",
       "[3051 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.drop([ 'Close'], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00414359],\n",
       "       [0.00365546],\n",
       "       [0.00321557],\n",
       "       ...,\n",
       "       [0.33536058],\n",
       "       [0.33451164],\n",
       "       [0.3365621 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Data normalization (cont.)\n",
    "scaler = MinMaxScaler()\n",
    "data_close = data[['Adj Close']]\n",
    "data_close = scaler.fit_transform(data_close)\n",
    "data_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Splitting the data\n",
    "X = data[['Open', 'High', 'Low', 'Volume']]\n",
    "X = scaler.fit_transform(X)\n",
    "y = data_close\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (3051, 4)\n",
      "y (3051, 1)\n",
      "X_train (2440, 1, 4)\n",
      "y_train (2440, 1)\n",
      "X_test (611, 1, 4)\n",
      "Y_test (611, 1)\n",
      "X_train.shape[1] 1\n",
      "X_train.shape[2] 4\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Reshaping the data\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(\"x\", X.shape)\n",
    "print('y',y.shape)\n",
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('Y_test', y_test.shape) \n",
    "print('X_train.shape[1]', X_train.shape[1])\n",
    "print('X_train.shape[2]', X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 1, 100)            31800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 100)            0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 100)               60600     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 92,501\n",
      "Trainable params: 92,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Building the GRU model (cont.)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True , activation= 'softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(100, activation= 'softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "77/77 [==============================] - 7s 16ms/step - loss: 0.0318 - val_loss: 0.2122\n",
      "Epoch 2/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0291 - val_loss: 0.2024\n",
      "Epoch 3/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0291 - val_loss: 0.2047\n",
      "Epoch 4/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0290 - val_loss: 0.2009\n",
      "Epoch 5/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0289 - val_loss: 0.1966\n",
      "Epoch 6/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0285 - val_loss: 0.1986\n",
      "Epoch 7/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0277 - val_loss: 0.1870\n",
      "Epoch 8/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0264 - val_loss: 0.1714\n",
      "Epoch 9/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0237 - val_loss: 0.1538\n",
      "Epoch 10/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0195 - val_loss: 0.1140\n",
      "Epoch 11/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0135 - val_loss: 0.0693\n",
      "Epoch 12/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0088 - val_loss: 0.0349\n",
      "Epoch 13/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0058 - val_loss: 0.0189\n",
      "Epoch 14/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0049 - val_loss: 0.0118\n",
      "Epoch 15/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0089\n",
      "Epoch 16/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0037 - val_loss: 0.0075\n",
      "Epoch 17/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0068\n",
      "Epoch 18/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0031 - val_loss: 0.0065\n",
      "Epoch 19/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0059\n",
      "Epoch 20/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0057\n",
      "Epoch 21/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0028 - val_loss: 0.0055\n",
      "Epoch 22/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 23/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 24/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 25/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0025 - val_loss: 0.0046\n",
      "Epoch 26/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0043\n",
      "Epoch 27/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0043\n",
      "Epoch 28/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0022 - val_loss: 0.0040\n",
      "Epoch 29/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0024 - val_loss: 0.0036\n",
      "Epoch 30/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 31/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0023 - val_loss: 0.0034\n",
      "Epoch 32/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 33/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 34/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0021 - val_loss: 0.0034\n",
      "Epoch 35/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 36/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 37/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 38/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 39/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 40/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 41/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 42/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 43/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 44/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 45/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 46/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 47/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 48/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 49/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 50/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 51/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 52/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 53/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 54/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 55/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 56/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 57/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 58/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 59/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 60/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 61/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 62/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 63/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 64/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 65/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 66/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 67/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 68/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 69/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 70/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 71/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 72/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 73/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 74/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.1137e-04 - val_loss: 0.0011\n",
      "Epoch 75/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 77/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.4860e-04 - val_loss: 9.9539e-04\n",
      "Epoch 78/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.6515e-04 - val_loss: 9.6943e-04\n",
      "Epoch 79/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.2376e-04 - val_loss: 9.8127e-04\n",
      "Epoch 80/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.8814e-04 - val_loss: 0.0013\n",
      "Epoch 81/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.7669e-04 - val_loss: 0.0011\n",
      "Epoch 82/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.8461e-04 - val_loss: 9.6349e-04\n",
      "Epoch 83/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.9298e-04 - val_loss: 9.9770e-04\n",
      "Epoch 84/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0010 - val_loss: 9.1278e-04\n",
      "Epoch 85/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.6506e-04 - val_loss: 9.8079e-04\n",
      "Epoch 86/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.8582e-04 - val_loss: 0.0011\n",
      "Epoch 87/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.9251e-04 - val_loss: 9.6024e-04\n",
      "Epoch 88/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.9267e-04 - val_loss: 0.0010\n",
      "Epoch 89/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.6870e-04 - val_loss: 8.8564e-04\n",
      "Epoch 90/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.9353e-04 - val_loss: 0.0010\n",
      "Epoch 91/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.4046e-04 - val_loss: 8.9928e-04\n",
      "Epoch 92/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.6635e-04 - val_loss: 8.0951e-04\n",
      "Epoch 93/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 94/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 9.2899e-04 - val_loss: 8.7949e-04\n",
      "Epoch 95/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.0007e-04 - val_loss: 8.7191e-04\n",
      "Epoch 96/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.6527e-04 - val_loss: 8.9193e-04\n",
      "Epoch 97/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 8.2618e-04 - val_loss: 9.7593e-04\n",
      "Epoch 98/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 8.3057e-04 - val_loss: 7.8469e-04\n",
      "Epoch 99/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.6110e-04 - val_loss: 0.0011\n",
      "Epoch 100/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.9221e-04 - val_loss: 7.9062e-04\n",
      "Epoch 101/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.8104e-04 - val_loss: 8.2416e-04\n",
      "Epoch 102/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.4042e-04 - val_loss: 9.5102e-04\n",
      "Epoch 103/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.9051e-04 - val_loss: 8.7487e-04\n",
      "Epoch 104/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.8483e-04 - val_loss: 8.5246e-04\n",
      "Epoch 105/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.6470e-04 - val_loss: 8.2160e-04\n",
      "Epoch 106/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.6407e-04 - val_loss: 8.5029e-04\n",
      "Epoch 107/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.6808e-04 - val_loss: 0.0010\n",
      "Epoch 108/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.5183e-04 - val_loss: 8.6934e-04\n",
      "Epoch 109/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.2560e-04 - val_loss: 8.4781e-04\n",
      "Epoch 110/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.3740e-04 - val_loss: 8.3302e-04\n",
      "Epoch 111/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.2374e-04 - val_loss: 7.5404e-04\n",
      "Epoch 112/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.9870e-04 - val_loss: 8.5366e-04\n",
      "Epoch 113/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.6301e-04 - val_loss: 7.4393e-04\n",
      "Epoch 114/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.8776e-04 - val_loss: 8.2851e-04\n",
      "Epoch 115/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.5692e-04 - val_loss: 8.4198e-04\n",
      "Epoch 116/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.1399e-04 - val_loss: 7.8744e-04\n",
      "Epoch 117/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.6041e-04 - val_loss: 7.4002e-04\n",
      "Epoch 118/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.2102e-04 - val_loss: 8.1503e-04\n",
      "Epoch 119/250\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 7.6222e-04 - val_loss: 8.9765e-04\n",
      "Epoch 120/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.3401e-04 - val_loss: 7.6995e-04\n",
      "Epoch 121/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.5683e-04 - val_loss: 8.0446e-04\n",
      "Epoch 122/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.0937e-04 - val_loss: 7.2136e-04\n",
      "Epoch 123/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.6408e-04 - val_loss: 9.7623e-04\n",
      "Epoch 124/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 8.6022e-04 - val_loss: 8.4817e-04\n",
      "Epoch 125/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.1065e-04 - val_loss: 8.7359e-04\n",
      "Epoch 126/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.5165e-04 - val_loss: 7.2216e-04\n",
      "Epoch 127/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.5217e-04 - val_loss: 9.9669e-04\n",
      "Epoch 128/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.5515e-04 - val_loss: 9.2361e-04\n",
      "Epoch 129/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.7819e-04 - val_loss: 8.8364e-04\n",
      "Epoch 130/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.9755e-04 - val_loss: 9.2578e-04\n",
      "Epoch 131/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.6554e-04 - val_loss: 7.1106e-04\n",
      "Epoch 132/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.3599e-04 - val_loss: 7.0440e-04\n",
      "Epoch 133/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.7734e-04 - val_loss: 7.6822e-04\n",
      "Epoch 134/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.4214e-04 - val_loss: 7.0939e-04\n",
      "Epoch 135/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.9087e-04 - val_loss: 7.0818e-04\n",
      "Epoch 136/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.8398e-04 - val_loss: 6.9163e-04\n",
      "Epoch 137/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.7113e-04 - val_loss: 7.2463e-04\n",
      "Epoch 138/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.9930e-04 - val_loss: 7.1636e-04\n",
      "Epoch 139/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.8610e-04 - val_loss: 7.8483e-04\n",
      "Epoch 140/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.3330e-04 - val_loss: 8.7192e-04\n",
      "Epoch 141/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.6638e-04 - val_loss: 8.8905e-04\n",
      "Epoch 142/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.8070e-04 - val_loss: 7.7911e-04\n",
      "Epoch 143/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 6.6181e-04 - val_loss: 7.0675e-04\n",
      "Epoch 144/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 6.1965e-04 - val_loss: 7.6221e-04\n",
      "Epoch 145/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 6.3337e-04 - val_loss: 9.1858e-04\n",
      "Epoch 146/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.7311e-04 - val_loss: 8.6496e-04\n",
      "Epoch 147/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.6647e-04 - val_loss: 7.4594e-04\n",
      "Epoch 148/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.5142e-04 - val_loss: 0.0010\n",
      "Epoch 149/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.1999e-04 - val_loss: 7.1851e-04\n",
      "Epoch 150/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.0325e-04 - val_loss: 8.0243e-04\n",
      "Epoch 151/250\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 5.6121e-04 - val_loss: 9.5530e-04\n",
      "Epoch 152/250\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 5.9829e-04 - val_loss: 0.0011\n",
      "Epoch 153/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.3525e-04 - val_loss: 7.9172e-04\n",
      "Epoch 154/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 6.6229e-04 - val_loss: 8.1534e-04\n",
      "Epoch 155/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.6845e-04 - val_loss: 7.7109e-04\n",
      "Epoch 156/250\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 5.7851e-04 - val_loss: 6.7025e-04\n",
      "Epoch 157/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.1852e-04 - val_loss: 7.0923e-04\n",
      "Epoch 158/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 6.1924e-04 - val_loss: 7.3345e-04\n",
      "Epoch 159/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.6566e-04 - val_loss: 8.8926e-04\n",
      "Epoch 160/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 6.2144e-04 - val_loss: 0.0010\n",
      "Epoch 161/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.1996e-04 - val_loss: 0.0013\n",
      "Epoch 162/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.8300e-04 - val_loss: 8.5830e-04\n",
      "Epoch 163/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.5419e-04 - val_loss: 7.2921e-04\n",
      "Epoch 164/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.0378e-04 - val_loss: 0.0010\n",
      "Epoch 165/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.4648e-04 - val_loss: 7.0156e-04\n",
      "Epoch 166/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.4666e-04 - val_loss: 7.2261e-04\n",
      "Epoch 167/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.1742e-04 - val_loss: 6.8552e-04\n",
      "Epoch 168/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.5529e-04 - val_loss: 7.6907e-04\n",
      "Epoch 169/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.7037e-04 - val_loss: 7.1380e-04\n",
      "Epoch 170/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.3035e-04 - val_loss: 7.4310e-04\n",
      "Epoch 171/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.7484e-04 - val_loss: 7.5002e-04\n",
      "Epoch 172/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.5404e-04 - val_loss: 7.3204e-04\n",
      "Epoch 173/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.0637e-04 - val_loss: 7.2791e-04\n",
      "Epoch 174/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.3331e-04 - val_loss: 7.1188e-04\n",
      "Epoch 175/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.7370e-04 - val_loss: 7.7008e-04\n",
      "Epoch 176/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.0752e-04 - val_loss: 7.0971e-04\n",
      "Epoch 177/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.0591e-04 - val_loss: 8.6031e-04\n",
      "Epoch 178/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.3329e-04 - val_loss: 8.3833e-04\n",
      "Epoch 179/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.9297e-04 - val_loss: 9.9662e-04\n",
      "Epoch 180/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.9823e-04 - val_loss: 7.3152e-04\n",
      "Epoch 181/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.4618e-04 - val_loss: 7.2484e-04\n",
      "Epoch 182/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.5971e-04 - val_loss: 7.2979e-04\n",
      "Epoch 183/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 5.7914e-04 - val_loss: 7.0972e-04\n",
      "Epoch 184/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.8346e-04 - val_loss: 7.4214e-04\n",
      "Epoch 185/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.4632e-04 - val_loss: 7.2070e-04\n",
      "Epoch 186/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.5358e-04 - val_loss: 8.1575e-04\n",
      "Epoch 187/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 4.9128e-04 - val_loss: 7.3952e-04\n",
      "Epoch 188/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.5065e-04 - val_loss: 7.2321e-04\n",
      "Epoch 189/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.6447e-04 - val_loss: 7.5421e-04\n",
      "Epoch 190/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 5.6473e-04 - val_loss: 9.3895e-04\n",
      "Epoch 191/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 4.9684e-04 - val_loss: 7.4201e-04\n",
      "Epoch 192/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 4.7410e-04 - val_loss: 7.0101e-04\n",
      "Epoch 193/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.7365e-04 - val_loss: 6.7943e-04\n",
      "Epoch 194/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.2864e-04 - val_loss: 8.8069e-04\n",
      "Epoch 195/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.0808e-04 - val_loss: 7.5728e-04\n",
      "Epoch 196/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.2544e-04 - val_loss: 9.4580e-04\n",
      "Epoch 197/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.4734e-04 - val_loss: 8.7464e-04\n",
      "Epoch 198/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.4980e-04 - val_loss: 7.3272e-04\n",
      "Epoch 199/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.0066e-04 - val_loss: 7.3890e-04\n",
      "Epoch 200/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.1622e-04 - val_loss: 8.3976e-04\n",
      "Epoch 201/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 4.1605e-04 - val_loss: 9.2193e-04\n",
      "Epoch 202/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.7014e-04 - val_loss: 6.8930e-04\n",
      "Epoch 203/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.9325e-04 - val_loss: 6.7872e-04\n",
      "Epoch 204/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.4741e-04 - val_loss: 7.7062e-04\n",
      "Epoch 205/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.4000e-04 - val_loss: 8.4273e-04\n",
      "Epoch 206/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.9627e-04 - val_loss: 6.9383e-04\n",
      "Epoch 207/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.5680e-04 - val_loss: 6.8740e-04\n",
      "Epoch 208/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.0583e-04 - val_loss: 8.2395e-04\n",
      "Epoch 209/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.3035e-04 - val_loss: 7.8992e-04\n",
      "Epoch 210/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.0021e-04 - val_loss: 6.6075e-04\n",
      "Epoch 211/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.5938e-04 - val_loss: 7.7121e-04\n",
      "Epoch 212/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.3764e-04 - val_loss: 6.8761e-04\n",
      "Epoch 213/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.4169e-04 - val_loss: 6.4659e-04\n",
      "Epoch 214/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 3.6066e-04 - val_loss: 6.4755e-04\n",
      "Epoch 215/250\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 4.7614e-04 - val_loss: 6.9369e-04\n",
      "Epoch 216/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 3.9541e-04 - val_loss: 8.4752e-04\n",
      "Epoch 217/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.6955e-04 - val_loss: 6.5995e-04\n",
      "Epoch 218/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.1791e-04 - val_loss: 6.5197e-04\n",
      "Epoch 219/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.0634e-04 - val_loss: 8.2871e-04\n",
      "Epoch 220/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.1909e-04 - val_loss: 6.7067e-04\n",
      "Epoch 221/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.7063e-04 - val_loss: 6.5780e-04\n",
      "Epoch 222/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.3770e-04 - val_loss: 6.8323e-04\n",
      "Epoch 223/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.5663e-04 - val_loss: 7.1078e-04\n",
      "Epoch 224/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.3057e-04 - val_loss: 6.9951e-04\n",
      "Epoch 225/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.7845e-04 - val_loss: 6.5238e-04\n",
      "Epoch 226/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.6963e-04 - val_loss: 6.0053e-04\n",
      "Epoch 227/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.1819e-04 - val_loss: 7.0887e-04\n",
      "Epoch 228/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.2645e-04 - val_loss: 6.4607e-04\n",
      "Epoch 229/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.1782e-04 - val_loss: 5.7491e-04\n",
      "Epoch 230/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.1413e-04 - val_loss: 5.6161e-04\n",
      "Epoch 231/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.7432e-04 - val_loss: 5.8080e-04\n",
      "Epoch 232/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.0852e-04 - val_loss: 6.5767e-04\n",
      "Epoch 233/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.6551e-04 - val_loss: 6.6546e-04\n",
      "Epoch 234/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.1892e-04 - val_loss: 6.6531e-04\n",
      "Epoch 235/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.2222e-04 - val_loss: 8.0424e-04\n",
      "Epoch 236/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.1543e-04 - val_loss: 5.5735e-04\n",
      "Epoch 237/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 3.6841e-04 - val_loss: 6.2421e-04\n",
      "Epoch 238/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.8812e-04 - val_loss: 6.0136e-04\n",
      "Epoch 239/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.6661e-04 - val_loss: 6.6820e-04\n",
      "Epoch 240/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 3.6157e-04 - val_loss: 7.3900e-04\n",
      "Epoch 241/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.3104e-04 - val_loss: 7.8697e-04\n",
      "Epoch 242/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.8603e-04 - val_loss: 8.8151e-04\n",
      "Epoch 243/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 3.3910e-04 - val_loss: 5.9087e-04\n",
      "Epoch 244/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.5051e-04 - val_loss: 5.8735e-04\n",
      "Epoch 245/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.0522e-04 - val_loss: 5.9600e-04\n",
      "Epoch 246/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.4159e-04 - val_loss: 5.6470e-04\n",
      "Epoch 247/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.1389e-04 - val_loss: 7.8106e-04\n",
      "Epoch 248/250\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 3.6604e-04 - val_loss: 6.8323e-04\n",
      "Epoch 249/250\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.3234e-04 - val_loss: 5.7443e-04\n",
      "Epoch 250/250\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.3584e-04 - val_loss: 7.7573e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training and evaluating the model\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 4ms/step\n",
      "y_pred (611, 1)\n",
      "y_test (611, 1)\n",
      "MAPE: 3.4881%\n",
      "RMSE: 0.0279\n",
      "MSE: 0.0008\n",
      "MAE:0.0205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 8: Tuning the Hyperparameters\n",
    "# You can use techniques such as grid search or random search to find the best values for the model's hyperparameters\n",
    "\n",
    "# Step 9: Testing the model\n",
    "y_pred = model.predict(X_test)\n",
    "print('y_pred',y_pred.shape)\n",
    "print('y_test',y_test.shape)\n",
    "#y_pred = y_pred.reshape(-1,1)\n",
    "#y_pred = np.nan_to_num(y_pred)\n",
    "#y_pred = scaler.inverse_transform(y_pred)\n",
    "#y_pred = scaler.inverse_transform(y_pred[:,[-1]])\n",
    "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "#y_test = np.nan_to_num(y_test)\n",
    "\n",
    "\n",
    "#evaluat\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = 100 * np.mean(np.abs((y_test - y_pred) / y_test))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAPE: {:.4f}%\".format(mape))\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"MSE: {:.4f}\".format(mse))\n",
    "print(\"MAE:{:.4f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smape: 3.4215%\n"
     ]
    }
   ],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
    "\n",
    "SMAPE1 = smape(y_test, y_pred)\n",
    "print(\"smape: {:.4f}%\".format(SMAPE1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (3051,) and (611, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Assume that you have stored the date information in a variable called 'date'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Assume that you have stored the predictions in a variable called 'y_pred'\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Assume that you have stored the test data in a variable called 'y_test'\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[39m# Plot the test data and predictions on a line chart\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m plt\u001b[39m.\u001b[39;49mplot(data\u001b[39m.\u001b[39;49mindex, y_pred, label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPredictions\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m plt\u001b[39m.\u001b[39mplot(data\u001b[39m.\u001b[39mindex, y_test, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest Data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:2740\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2738\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2739\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2740\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   2741\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   2742\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1662\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1663\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (3051,) and (611, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume that you have stored the date information in a variable called 'date'\n",
    "# Assume that you have stored the predictions in a variable called 'y_pred'\n",
    "# Assume that you have stored the test data in a variable called 'y_test'\n",
    "\n",
    "# Plot the test data and predictions on a line chart\n",
    "plt.plot(data.index, y_pred, label='Predictions')\n",
    "plt.plot(data.index, y_test, label='Test Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (611,1) doesn't match the broadcast shape (611,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49minverse_transform(y_pred)\n\u001b[0;32m      2\u001b[0m \u001b[39m#y_pred = scaler.inverse_transform(y_pred[:,[-1]])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m'\u001b[39m,y_pred)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:541\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    535\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    537\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    538\u001b[0m     X, copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy, dtype\u001b[39m=\u001b[39mFLOAT_DTYPES, force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m )\n\u001b[1;32m--> 541\u001b[0m X \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n\u001b[0;32m    542\u001b[0m X \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n\u001b[0;32m    543\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (611,1) doesn't match the broadcast shape (611,4)"
     ]
    }
   ],
   "source": [
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "#y_pred = scaler.inverse_transform(y_pred[:,[-1]])\n",
    "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "print ('y_pred',y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m,\u001b[39m8\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mplot(y_test, color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m, label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(y_pred, color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m, label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(y_test, color = 'blue', label = 'Test')\n",
    "plt.plot(y_pred, color = 'red', label = 'pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67e0cbc25fa4f5baaacba1240f401bc655b640f8e15cfc935dfee2e63491bdf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
