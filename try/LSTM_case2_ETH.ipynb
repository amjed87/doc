{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-09</th>\n",
       "      <td>308.644989</td>\n",
       "      <td>329.451996</td>\n",
       "      <td>307.056000</td>\n",
       "      <td>320.884003</td>\n",
       "      <td>320.884003</td>\n",
       "      <td>893249984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-10</th>\n",
       "      <td>320.670990</td>\n",
       "      <td>324.717987</td>\n",
       "      <td>294.541992</td>\n",
       "      <td>299.252991</td>\n",
       "      <td>299.252991</td>\n",
       "      <td>885985984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11</th>\n",
       "      <td>298.585999</td>\n",
       "      <td>319.453003</td>\n",
       "      <td>298.191986</td>\n",
       "      <td>314.681000</td>\n",
       "      <td>314.681000</td>\n",
       "      <td>842300992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-12</th>\n",
       "      <td>314.690002</td>\n",
       "      <td>319.153015</td>\n",
       "      <td>298.513000</td>\n",
       "      <td>307.907990</td>\n",
       "      <td>307.907990</td>\n",
       "      <td>1613479936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-13</th>\n",
       "      <td>307.024994</td>\n",
       "      <td>328.415009</td>\n",
       "      <td>307.024994</td>\n",
       "      <td>316.716003</td>\n",
       "      <td>316.716003</td>\n",
       "      <td>1041889984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>1515.249634</td>\n",
       "      <td>1557.970337</td>\n",
       "      <td>1514.380005</td>\n",
       "      <td>1552.556519</td>\n",
       "      <td>1552.556519</td>\n",
       "      <td>6432638856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-20</th>\n",
       "      <td>1552.373657</td>\n",
       "      <td>1659.885742</td>\n",
       "      <td>1544.917847</td>\n",
       "      <td>1659.754150</td>\n",
       "      <td>1659.754150</td>\n",
       "      <td>8528894754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-21</th>\n",
       "      <td>1659.706055</td>\n",
       "      <td>1674.179321</td>\n",
       "      <td>1626.812988</td>\n",
       "      <td>1627.118164</td>\n",
       "      <td>1627.118164</td>\n",
       "      <td>8859250310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-22</th>\n",
       "      <td>1627.365967</td>\n",
       "      <td>1658.023438</td>\n",
       "      <td>1612.085693</td>\n",
       "      <td>1628.382080</td>\n",
       "      <td>1628.382080</td>\n",
       "      <td>7517988734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-23</th>\n",
       "      <td>1627.538208</td>\n",
       "      <td>1640.932129</td>\n",
       "      <td>1612.744141</td>\n",
       "      <td>1619.800903</td>\n",
       "      <td>1619.800903</td>\n",
       "      <td>7906475008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2017-11-09   308.644989   329.451996   307.056000   320.884003   320.884003   \n",
       "2017-11-10   320.670990   324.717987   294.541992   299.252991   299.252991   \n",
       "2017-11-11   298.585999   319.453003   298.191986   314.681000   314.681000   \n",
       "2017-11-12   314.690002   319.153015   298.513000   307.907990   307.907990   \n",
       "2017-11-13   307.024994   328.415009   307.024994   316.716003   316.716003   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2023-01-19  1515.249634  1557.970337  1514.380005  1552.556519  1552.556519   \n",
       "2023-01-20  1552.373657  1659.885742  1544.917847  1659.754150  1659.754150   \n",
       "2023-01-21  1659.706055  1674.179321  1626.812988  1627.118164  1627.118164   \n",
       "2023-01-22  1627.365967  1658.023438  1612.085693  1628.382080  1628.382080   \n",
       "2023-01-23  1627.538208  1640.932129  1612.744141  1619.800903  1619.800903   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2017-11-09   893249984  \n",
       "2017-11-10   885985984  \n",
       "2017-11-11   842300992  \n",
       "2017-11-12  1613479936  \n",
       "2017-11-13  1041889984  \n",
       "...                ...  \n",
       "2023-01-19  6432638856  \n",
       "2023-01-20  8528894754  \n",
       "2023-01-21  8859250310  \n",
       "2023-01-22  7517988734  \n",
       "2023-01-23  7906475008  \n",
       "\n",
       "[1902 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 1: Data cleaning\n",
    "data = yf.download(tickers='ETH-usd', period='max', interval='1d')\n",
    "data.dropna(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EMA_short</th>\n",
       "      <th>EMA_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-16</td>\n",
       "      <td>934.786011</td>\n",
       "      <td>950.005005</td>\n",
       "      <td>917.848022</td>\n",
       "      <td>944.210022</td>\n",
       "      <td>2369449984</td>\n",
       "      <td>50.673576</td>\n",
       "      <td>918.268825</td>\n",
       "      <td>763.084961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-17</td>\n",
       "      <td>944.747986</td>\n",
       "      <td>976.594971</td>\n",
       "      <td>940.754028</td>\n",
       "      <td>974.114990</td>\n",
       "      <td>2525720064</td>\n",
       "      <td>52.428326</td>\n",
       "      <td>923.587508</td>\n",
       "      <td>767.263773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>973.348999</td>\n",
       "      <td>982.932983</td>\n",
       "      <td>915.445007</td>\n",
       "      <td>923.921021</td>\n",
       "      <td>2567290112</td>\n",
       "      <td>49.275923</td>\n",
       "      <td>923.619271</td>\n",
       "      <td>770.365897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-19</td>\n",
       "      <td>921.668030</td>\n",
       "      <td>957.776978</td>\n",
       "      <td>921.554016</td>\n",
       "      <td>943.864990</td>\n",
       "      <td>2169019904</td>\n",
       "      <td>50.541930</td>\n",
       "      <td>925.547435</td>\n",
       "      <td>773.801523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-20</td>\n",
       "      <td>943.567017</td>\n",
       "      <td>965.265015</td>\n",
       "      <td>892.953979</td>\n",
       "      <td>895.370972</td>\n",
       "      <td>2545260032</td>\n",
       "      <td>47.456221</td>\n",
       "      <td>922.673486</td>\n",
       "      <td>776.208839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>1515.249634</td>\n",
       "      <td>1557.970337</td>\n",
       "      <td>1514.380005</td>\n",
       "      <td>1552.556519</td>\n",
       "      <td>6432638856</td>\n",
       "      <td>74.087366</td>\n",
       "      <td>1411.743041</td>\n",
       "      <td>1344.826320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>2023-01-20</td>\n",
       "      <td>1552.373657</td>\n",
       "      <td>1659.885742</td>\n",
       "      <td>1544.917847</td>\n",
       "      <td>1659.754150</td>\n",
       "      <td>8528894754</td>\n",
       "      <td>79.862386</td>\n",
       "      <td>1435.363147</td>\n",
       "      <td>1351.062514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>2023-01-21</td>\n",
       "      <td>1659.706055</td>\n",
       "      <td>1674.179321</td>\n",
       "      <td>1626.812988</td>\n",
       "      <td>1627.118164</td>\n",
       "      <td>8859250310</td>\n",
       "      <td>74.450085</td>\n",
       "      <td>1453.625530</td>\n",
       "      <td>1356.528963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>1627.365967</td>\n",
       "      <td>1658.023438</td>\n",
       "      <td>1612.085693</td>\n",
       "      <td>1628.382080</td>\n",
       "      <td>7517988734</td>\n",
       "      <td>74.521732</td>\n",
       "      <td>1470.269011</td>\n",
       "      <td>1361.912193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>1627.538208</td>\n",
       "      <td>1640.932129</td>\n",
       "      <td>1612.744141</td>\n",
       "      <td>1619.800903</td>\n",
       "      <td>7906475008</td>\n",
       "      <td>73.031994</td>\n",
       "      <td>1484.510143</td>\n",
       "      <td>1367.018900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1803 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Open         High          Low    Adj Close  \\\n",
       "0    2018-02-16   934.786011   950.005005   917.848022   944.210022   \n",
       "1    2018-02-17   944.747986   976.594971   940.754028   974.114990   \n",
       "2    2018-02-18   973.348999   982.932983   915.445007   923.921021   \n",
       "3    2018-02-19   921.668030   957.776978   921.554016   943.864990   \n",
       "4    2018-02-20   943.567017   965.265015   892.953979   895.370972   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "1798 2023-01-19  1515.249634  1557.970337  1514.380005  1552.556519   \n",
       "1799 2023-01-20  1552.373657  1659.885742  1544.917847  1659.754150   \n",
       "1800 2023-01-21  1659.706055  1674.179321  1626.812988  1627.118164   \n",
       "1801 2023-01-22  1627.365967  1658.023438  1612.085693  1628.382080   \n",
       "1802 2023-01-23  1627.538208  1640.932129  1612.744141  1619.800903   \n",
       "\n",
       "          Volume        RSI    EMA_short     EMA_long  \n",
       "0     2369449984  50.673576   918.268825   763.084961  \n",
       "1     2525720064  52.428326   923.587508   767.263773  \n",
       "2     2567290112  49.275923   923.619271   770.365897  \n",
       "3     2169019904  50.541930   925.547435   773.801523  \n",
       "4     2545260032  47.456221   922.673486   776.208839  \n",
       "...          ...        ...          ...          ...  \n",
       "1798  6432638856  74.087366  1411.743041  1344.826320  \n",
       "1799  8528894754  79.862386  1435.363147  1351.062514  \n",
       "1800  8859250310  74.450085  1453.625530  1356.528963  \n",
       "1801  7517988734  74.521732  1470.269011  1361.912193  \n",
       "1802  7906475008  73.031994  1484.510143  1367.018900  \n",
       "\n",
       "[1803 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Feature engineering\n",
    "data['RSI'] = ta.rsi(data.Close, length=15)\n",
    "data['EMA_short'] = ta.ema(data.Close, length=20)\n",
    "data['EMA_long'] = ta.ema(data.Close, length=100)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace = True)\n",
    "data.drop([ 'Close'], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18188281],\n",
       "       [0.18820818],\n",
       "       [0.17759136],\n",
       "       ...,\n",
       "       [0.32632867],\n",
       "       [0.32659601],\n",
       "       [0.32478095]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Data normalization (cont.)\n",
    "#scaler = MinMaxScaler()\n",
    "#data[['Open', 'High', 'Low', 'Volume', 'Close', 'RSI', 'EMA_short', 'EMA_long']] = scaler.fit_transform(data[['Open', 'High', 'Low', 'Volume', 'Close', 'RSI', 'EMA_short', 'EMA_long']])\n",
    "#data\n",
    "scaler = MinMaxScaler()\n",
    "data_close = data[['Adj Close']]\n",
    "data_close = scaler.fit_transform(data_close)\n",
    "data_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[0.17997119 0.17989954 0.18014681 ... 0.46581789 0.18611508 0.16402886]\n",
      " [0.18207919 0.18543178 0.18508855 ... 0.48990784 0.18732707 0.16512842]\n",
      " [0.1881313  0.18675045 0.17962838 ... 0.44663031 0.18733431 0.16594467]\n",
      " ...\n",
      " [0.33336772 0.33056949 0.33309891 ... 0.79223187 0.3081092  0.32017984]\n",
      " [0.3265244  0.32720813 0.32992164 ... 0.79321546 0.31190182 0.32159631]\n",
      " [0.32656085 0.32365216 0.33006369 ... 0.77276371 0.31514701 0.32294002]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Splitting the data\n",
    "X = data[['Open', 'High', 'Low', 'Volume', 'RSI', 'EMA_short', 'EMA_long']]\n",
    "X = scaler.fit_transform(X)\n",
    "y = data_close\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "print('X', X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (1803, 7)\n",
      "y (1803, 1)\n",
      "X_train (1442, 1, 7)\n",
      "y_train (1442, 1)\n",
      "X_test (361, 1, 7)\n",
      "Y_test (361, 1)\n",
      "X_train.shape[1] 1\n",
      "X_train.shape[2] 7\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Reshaping the data\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(\"x\", X.shape)\n",
    "print('y',y.shape)\n",
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('Y_test', y_test.shape) \n",
    "print('X_train.shape[1]', X_train.shape[1])\n",
    "print('X_train.shape[2]', X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 100)            43200     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 100)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123,701\n",
      "Trainable params: 123,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Building the LSTM model (cont.)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True , activation= 'softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, activation= 'softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "46/46 [==============================] - 7s 30ms/step - loss: 0.0899 - val_loss: 0.1117\n",
      "Epoch 2/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0763 - val_loss: 0.0846\n",
      "Epoch 3/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0710 - val_loss: 0.0701\n",
      "Epoch 4/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0691 - val_loss: 0.0618\n",
      "Epoch 5/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0688 - val_loss: 0.0596\n",
      "Epoch 6/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0687 - val_loss: 0.0589\n",
      "Epoch 7/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0688 - val_loss: 0.0590\n",
      "Epoch 8/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0688 - val_loss: 0.0591\n",
      "Epoch 9/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0686 - val_loss: 0.0582\n",
      "Epoch 10/250\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0684 - val_loss: 0.0582\n",
      "Epoch 11/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0685 - val_loss: 0.0585\n",
      "Epoch 12/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0685 - val_loss: 0.0578\n",
      "Epoch 13/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0684 - val_loss: 0.0578\n",
      "Epoch 14/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0681 - val_loss: 0.0570\n",
      "Epoch 15/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0680 - val_loss: 0.0573\n",
      "Epoch 16/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0678 - val_loss: 0.0571\n",
      "Epoch 17/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0676 - val_loss: 0.0571\n",
      "Epoch 18/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0675 - val_loss: 0.0560\n",
      "Epoch 19/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0670 - val_loss: 0.0546\n",
      "Epoch 20/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0667 - val_loss: 0.0549\n",
      "Epoch 21/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0662 - val_loss: 0.0551\n",
      "Epoch 22/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0660 - val_loss: 0.0538\n",
      "Epoch 23/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0657 - val_loss: 0.0534\n",
      "Epoch 24/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0651 - val_loss: 0.0528\n",
      "Epoch 25/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0646 - val_loss: 0.0526\n",
      "Epoch 26/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0639 - val_loss: 0.0517\n",
      "Epoch 27/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0631 - val_loss: 0.0497\n",
      "Epoch 28/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0622 - val_loss: 0.0497\n",
      "Epoch 29/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0613 - val_loss: 0.0482\n",
      "Epoch 30/250\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0606 - val_loss: 0.0461\n",
      "Epoch 31/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0594 - val_loss: 0.0447\n",
      "Epoch 32/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0583 - val_loss: 0.0435\n",
      "Epoch 33/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0570 - val_loss: 0.0427\n",
      "Epoch 34/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0556 - val_loss: 0.0404\n",
      "Epoch 35/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0538 - val_loss: 0.0382\n",
      "Epoch 36/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0518 - val_loss: 0.0366\n",
      "Epoch 37/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0505 - val_loss: 0.0352\n",
      "Epoch 38/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0485 - val_loss: 0.0327\n",
      "Epoch 39/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0465 - val_loss: 0.0306\n",
      "Epoch 40/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0446 - val_loss: 0.0291\n",
      "Epoch 41/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0423 - val_loss: 0.0276\n",
      "Epoch 42/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0402 - val_loss: 0.0250\n",
      "Epoch 43/250\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0380 - val_loss: 0.0230\n",
      "Epoch 44/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0357 - val_loss: 0.0205\n",
      "Epoch 45/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0332 - val_loss: 0.0187\n",
      "Epoch 46/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0306 - val_loss: 0.0169\n",
      "Epoch 47/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0294 - val_loss: 0.0155\n",
      "Epoch 48/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0272 - val_loss: 0.0139\n",
      "Epoch 49/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0126\n",
      "Epoch 50/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0227 - val_loss: 0.0114\n",
      "Epoch 51/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0217 - val_loss: 0.0104\n",
      "Epoch 52/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0198 - val_loss: 0.0096\n",
      "Epoch 53/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0183 - val_loss: 0.0088\n",
      "Epoch 54/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0173 - val_loss: 0.0083\n",
      "Epoch 55/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0158 - val_loss: 0.0079\n",
      "Epoch 56/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 0.0076\n",
      "Epoch 57/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0074\n",
      "Epoch 58/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0125 - val_loss: 0.0072\n",
      "Epoch 59/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0119 - val_loss: 0.0070\n",
      "Epoch 60/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0107 - val_loss: 0.0070\n",
      "Epoch 61/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0103 - val_loss: 0.0072\n",
      "Epoch 62/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0070\n",
      "Epoch 63/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0070\n",
      "Epoch 64/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0087 - val_loss: 0.0070\n",
      "Epoch 65/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 66/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 67/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 0.0073\n",
      "Epoch 68/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 69/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0076\n",
      "Epoch 70/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0067 - val_loss: 0.0078\n",
      "Epoch 71/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0068 - val_loss: 0.0079\n",
      "Epoch 72/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0081\n",
      "Epoch 73/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 74/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 75/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 76/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.0087\n",
      "Epoch 77/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0089\n",
      "Epoch 78/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0089\n",
      "Epoch 79/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0090\n",
      "Epoch 80/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0057 - val_loss: 0.0091\n",
      "Epoch 81/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0091\n",
      "Epoch 82/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0056 - val_loss: 0.0091\n",
      "Epoch 83/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0055 - val_loss: 0.0092\n",
      "Epoch 84/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0092\n",
      "Epoch 85/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0054 - val_loss: 0.0091\n",
      "Epoch 86/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0091\n",
      "Epoch 87/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0090\n",
      "Epoch 88/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0091\n",
      "Epoch 89/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0090\n",
      "Epoch 90/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0048 - val_loss: 0.0090\n",
      "Epoch 91/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0089\n",
      "Epoch 92/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0087\n",
      "Epoch 93/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 94/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0084\n",
      "Epoch 95/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0081\n",
      "Epoch 96/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0079\n",
      "Epoch 97/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0077\n",
      "Epoch 98/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0075\n",
      "Epoch 99/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0073\n",
      "Epoch 100/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0043 - val_loss: 0.0071\n",
      "Epoch 101/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0043 - val_loss: 0.0070\n",
      "Epoch 102/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0068\n",
      "Epoch 103/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0066\n",
      "Epoch 104/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0065\n",
      "Epoch 105/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0041 - val_loss: 0.0061\n",
      "Epoch 106/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0060\n",
      "Epoch 107/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0058\n",
      "Epoch 108/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 109/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 110/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0051\n",
      "Epoch 111/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0037 - val_loss: 0.0050\n",
      "Epoch 112/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 113/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 114/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 115/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 116/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 117/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 118/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 119/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 120/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0037\n",
      "Epoch 121/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 122/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 123/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 124/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 125/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 126/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 127/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 128/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 129/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 130/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 131/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 132/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 133/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 134/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 135/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 136/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 137/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 138/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 139/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 140/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 141/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 142/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 143/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 144/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 145/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 146/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 147/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 148/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 149/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 150/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 151/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 152/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 153/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 154/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 155/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 156/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 157/250\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 158/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 159/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 160/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 161/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 162/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 163/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 164/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 165/250\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 166/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 167/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 9.9443e-04\n",
      "Epoch 168/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 169/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0019 - val_loss: 0.0010\n",
      "Epoch 170/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 171/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 9.7340e-04\n",
      "Epoch 172/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0018 - val_loss: 8.9183e-04\n",
      "Epoch 173/250\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.0017 - val_loss: 9.3465e-04\n",
      "Epoch 174/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 8.4121e-04\n",
      "Epoch 175/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 9.6574e-04\n",
      "Epoch 176/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 8.1250e-04\n",
      "Epoch 177/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 8.3452e-04\n",
      "Epoch 178/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 8.2966e-04\n",
      "Epoch 179/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 8.2936e-04\n",
      "Epoch 180/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 7.1341e-04\n",
      "Epoch 181/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0019 - val_loss: 7.8802e-04\n",
      "Epoch 182/250\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.0017 - val_loss: 8.0386e-04\n",
      "Epoch 183/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 7.9875e-04\n",
      "Epoch 184/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 7.3704e-04\n",
      "Epoch 185/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 6.7418e-04\n",
      "Epoch 186/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 8.1313e-04\n",
      "Epoch 187/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 7.4093e-04\n",
      "Epoch 188/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 6.7354e-04\n",
      "Epoch 189/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 7.5881e-04\n",
      "Epoch 190/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 6.2661e-04\n",
      "Epoch 191/250\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0018 - val_loss: 6.0196e-04\n",
      "Epoch 192/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0018 - val_loss: 6.7471e-04\n",
      "Epoch 193/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 6.1788e-04\n",
      "Epoch 194/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 6.0171e-04\n",
      "Epoch 195/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 5.6625e-04\n",
      "Epoch 196/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 6.0403e-04\n",
      "Epoch 197/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 6.0645e-04\n",
      "Epoch 198/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 5.6786e-04\n",
      "Epoch 199/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 5.7427e-04\n",
      "Epoch 200/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 5.5360e-04\n",
      "Epoch 201/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 6.0814e-04\n",
      "Epoch 202/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 5.1572e-04\n",
      "Epoch 203/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 5.5697e-04\n",
      "Epoch 204/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 5.8617e-04\n",
      "Epoch 205/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 4.9037e-04\n",
      "Epoch 206/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0015 - val_loss: 5.0363e-04\n",
      "Epoch 207/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 5.0043e-04\n",
      "Epoch 208/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 5.4135e-04\n",
      "Epoch 209/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 5.0193e-04\n",
      "Epoch 210/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 5.0126e-04\n",
      "Epoch 211/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 5.6001e-04\n",
      "Epoch 212/250\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0015 - val_loss: 4.6566e-04\n",
      "Epoch 213/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 4.4335e-04\n",
      "Epoch 214/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 4.7647e-04\n",
      "Epoch 215/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 4.4585e-04\n",
      "Epoch 216/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 4.3920e-04\n",
      "Epoch 217/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 4.3613e-04\n",
      "Epoch 218/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 5.6923e-04\n",
      "Epoch 219/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 4.4462e-04\n",
      "Epoch 220/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 4.2104e-04\n",
      "Epoch 221/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 4.3070e-04\n",
      "Epoch 222/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0014 - val_loss: 4.1146e-04\n",
      "Epoch 223/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 4.3114e-04\n",
      "Epoch 224/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 4.5699e-04\n",
      "Epoch 225/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 5.1720e-04\n",
      "Epoch 226/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 4.0235e-04\n",
      "Epoch 227/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 4.1061e-04\n",
      "Epoch 228/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 3.8901e-04\n",
      "Epoch 229/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 4.1817e-04\n",
      "Epoch 230/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0016 - val_loss: 3.9114e-04\n",
      "Epoch 231/250\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0015 - val_loss: 5.1891e-04\n",
      "Epoch 232/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 4.3941e-04\n",
      "Epoch 233/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 3.6761e-04\n",
      "Epoch 234/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 3.6665e-04\n",
      "Epoch 235/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 3.8868e-04\n",
      "Epoch 236/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 3.8422e-04\n",
      "Epoch 237/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0015 - val_loss: 3.7229e-04\n",
      "Epoch 238/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 3.9708e-04\n",
      "Epoch 239/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 4.2229e-04\n",
      "Epoch 240/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 3.8454e-04\n",
      "Epoch 241/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 4.0468e-04\n",
      "Epoch 242/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 3.8289e-04\n",
      "Epoch 243/250\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 4.0416e-04\n",
      "Epoch 244/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 3.6978e-04\n",
      "Epoch 245/250\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.0013 - val_loss: 4.1552e-04\n",
      "Epoch 246/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 3.7430e-04\n",
      "Epoch 247/250\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.0012 - val_loss: 3.7461e-04\n",
      "Epoch 248/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 3.6557e-04\n",
      "Epoch 249/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 3.8242e-04\n",
      "Epoch 250/250\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 4.0073e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training and evaluating the model\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 4ms/step\n",
      "y_pred (361, 1)\n",
      "y_test (361, 1)\n",
      "MAPE: 5.0536%\n",
      "RMSE: 0.0200\n",
      "MSE: 0.0004\n",
      "MAE:0.0154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 8: Tuning the Hyperparameters\n",
    "# You can use techniques such as grid search or random search to find the best values for the model's hyperparameters\n",
    "\n",
    "# Step 9: Testing the model\n",
    "y_pred = model.predict(X_test)\n",
    "print('y_pred',y_pred.shape)\n",
    "print('y_test',y_test.shape)\n",
    "#y_pred = y_pred.reshape(-1,1)\n",
    "#y_pred = np.nan_to_num(y_pred)\n",
    "#y_pred = scaler.inverse_transform(y_pred)\n",
    "#y_pred = scaler.inverse_transform(y_pred[:,[-1]])\n",
    "#y_test = scaler.inverse_transform(y_test)\n",
    "#y_test = np.nan_to_num(y_test)\n",
    "\n",
    "\n",
    "#evaluat\n",
    "mse = mean_squared_error(y_test, y_pred )\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = 100 * np.mean(np.abs((y_test - y_pred) / y_test))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"MAPE: {:.4f}%\".format(mape))\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"MSE: {:.4f}\".format(mse))\n",
    "print(\"MAE:{:.4f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 5.0536%\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "def mape(y_true, y_pred):\n",
    "    return 100 * np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "mape_value = mape(y_test, y_pred)\n",
    "print(f'MAPE: {mape_value:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smape: 4.8236%\n"
     ]
    }
   ],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    return 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
    "\n",
    "SMAPE1 = smape(y_test, y_pred)\n",
    "print(\"smape: {:.4f}%\".format(SMAPE1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1803,) and (361, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m,\u001b[39m7\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39;49mplot(data[\u001b[39m'\u001b[39;49m\u001b[39mDate\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues, y_test, color \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mred\u001b[39;49m\u001b[39m'\u001b[39;49m, label \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mReal Bitcoin Price\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(data[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, y_pred, color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m, label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPredicted Bitcoin Price\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mxticks(np\u001b[39m.\u001b[39marange(\u001b[39m100\u001b[39m,data[\u001b[39m1800\u001b[39m:]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m20\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:2740\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2738\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2739\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2740\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   2741\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   2742\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1662\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1663\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1803,) and (361, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "plt.plot(data['Date'].values, y_test, color = 'red', label = 'Real Bitcoin Price')\n",
    "plt.plot(data['Date'].values, y_pred, color = 'blue', label = 'Predicted Bitcoin Price')\n",
    "plt.xticks(np.arange(100,data[1800:].shape[0],20))\n",
    "plt.title('Bitcoin Price Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67e0cbc25fa4f5baaacba1240f401bc655b640f8e15cfc935dfee2e63491bdf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
